<div class="container">

<table style="width: 100%;"><tr>
<td>videoFaceAnalysis</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Analyze the facial features within an exported Zoom video file</h2>

<h3>Description</h3>

<p>Using this function you can analyze attributes of facial expressions within
a video file. There are two ways to supply the video information. First, you
can provide the actual video file. The function will then break it down
into still frames using the grabVideoStills() function. Second, you can use
the videoImageDirectory argument to give the location of a directory where
images have been pre-saved.
</p>


<h3>Usage</h3>

<pre><code class="language-R">videoFaceAnalysis(
  inputVideo,
  recordingStartDateTime,
  sampleWindow,
  facesCollectionID = NA,
  videoImageDirectory = NULL,
  grabVideoStills = FALSE,
  overWriteDir = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>inputVideo</code></td>
<td>
<p>string path to the video file (ideal is gallery)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recordingStartDateTime</code></td>
<td>
<p>YYYY-MM-DD HH:MM:SS of the start of the recording</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sampleWindow</code></td>
<td>
<p>Frame rate for the analysis</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>facesCollectionID</code></td>
<td>
<p>name of an 'AWS' collection with identified faces</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>videoImageDirectory</code></td>
<td>
<p>path to a directory that either contains image files or where you want to save image files</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>grabVideoStills</code></td>
<td>
<p>logical indicating whether you want the function to split the video file or not</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>overWriteDir</code></td>
<td>
<p>logical indicating whether to overwrite videoImageDirectory if it exists</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>data.frame with one record for every face detected in each frame. For each face, there is an abundance of information from 'AWS Rekognition'. This output is quite detailed. Note that there will be a varying number of faces per sampled frame in the video. Imagine that you have sampled the meeting and had someone rate each person's face within that sampled moment.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
vid.out = videoFaceAnalysis(inputVideo="meeting001_video.mp4", 
recordingStartDateTime="2020-04-20 13:30:00", 
sampleWindow=1, facesCollectionID="group-r",
videoImageDirectory="~/Documents/meetingImages", 
grabVideoStills=FALSE, overWriteDir=FALSE)

## End(Not run)
</code></pre>


</div>